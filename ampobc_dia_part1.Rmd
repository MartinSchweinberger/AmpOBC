---
title: "Analyzing change in the British English Adjective Amplifier System based on the Old Baily Corpus (Part 1)"
author: "Martin Schweinberger"
date: "`r format(Sys.time(), '%Y-%m-%d')`"
output:
  bookdown::html_document2: default
bibliography: bibliography.bib
link-citations: yes
---

```{r uq1, echo=F, fig.cap="", message=FALSE, warning=FALSE, out.width='100%'}
knitr::include_graphics("https://slcladal.github.io/images/isle6.png")
```


This document reports the analysis of the use of adjective amplifiers in British English based on the [Old Bailey Corpus (Version 2.0)](https://fedora.clarin-d.uni-saarland.de/oldbailey/).

# Introduction

We will now begin with the analysis. In a first step, the session is prepared by setting options and activating packages.

```{r ampobc_1_01, message=F, warning=F}
# load packages
library(tidyverse)
library(tidyr)
library(knitr) 
library(kableExtra) 
library(DT)
library(usethis)
library(textreadr)
# set options
options(stringsAsFactors = F)
options(scipen = 999)
options(max.print=10000)
# specify paths
corpuspath <- "D:\\Uni\\Korpora\\Original\\OldBaileyCorpus\\OldBaileyCorpus2\\OldBaileyCorpus2\\OBC2"
# define corpus files
corpusfiles = list.files(path = corpuspath, pattern = ".xml", all.files = T,
                         full.names = T, recursive = F, ignore.case = T, 
                         include.dirs = F)
# for testing purposes we only use the first 5 files
#corpusfiles <- corpusfiles[1:5]
head(corpusfiles)
```

# Data Processing

Load and process data

```{r ampobc_1_02, message=F, warning=F}
# load and start processing corpus
obc <- sapply(corpusfiles, function(x) {
  x <- textreadr::read_xml(x)
  x <- paste0(x, collapse = " ")
  x <- stringr::str_remove_all(x, "\n")
  x <- stringr::str_squish(x)
  x <- stringr::str_replace_all(x, " ([:punct:])", "\\1")
} ) 
# inspect
str(obc)
```




```{r ampobc_1_02, message=F, warning=F}
# extract file names
File <- names(obc)
File <- stringr::str_remove_all(File, ".*/")
File <- gsub(".xml", "", File)
File
```


```{r ampobc_1_02, message=F, warning=F}
# extract raw text
Text <- as.vector(unlist(obc))
# clean text
CleanText <- Text %>%
  stringr::str_remove_all(fixed("\\")) %>%
  tolower() %>%
  stringr::str_squish()
# combine elements into a data frame 
obcdf <- data.frame(File, Text, CleanText)
# remove problematic elements
#obcdf <- obcdf[-grep("\\[", obcdf$CleanText),]
# remove empty speech units
obcdf <- obcdf[obcdf$CleanText != "",]
nrow(obcdf)
```

Extract word counts

```{r ampobc_1_03, message=F, warning=F}
# add wordcount
obcdf <- obcdf %>%
  dplyr::mutate(WordCount = str_count(CleanText, " ")+1)
# inspect
colnames(obcdf)
```


Save data to disc

```{r ampobc_1_04, message=F, warning=F}
# save raw data to disc
write.table(obcdf, here::here("data", "obcdf_raw.txt"), sep = "\t", row.names = F, col.names = T)
obcdf <- read.delim(here::here("data", "obcdf_raw.txt"), sep = "\t", header = T, skipNul = T)
```

## PoS-Tagging{-}

Split data

```{r ampobc_1_05, message=F, warning=F}
# split data into smaller chunks
pos01 <- obcdf$CleanText[1:100]
pos02 <- obcdf$CleanText[101:200]
pos03 <- obcdf$CleanText[201:300]
pos04 <- obcdf$CleanText[301:400]
pos05 <- obcdf$CleanText[401:500]
pos06 <- obcdf$CleanText[501:600]
pos07 <- obcdf$CleanText[601:700]
pos08 <- obcdf$CleanText[701:800]
pos09 <- obcdf$CleanText[801:900]
pos10 <- obcdf$CleanText[901:1000]
pos11 <- obcdf$CleanText[1001:1100]
pos12 <- obcdf$CleanText[1101:1200]
pos13 <- obcdf$CleanText[1201:nrow(obcdf)]
```

Pos-tagging

```{r ampobc_1_06, message=F, warning=F, eval = F}
source("D:\\R/POStagObject.R") # for pos-tagging objects in R
library(NLP)
library(openNLP)
# pos tagging data
obcpos01 <- POStag(object = pos01)
obcpos01 <- as.vector(unlist(obcpos01))
writeLines(obcpos01, con = here::here("data", "obcpos01.txt"), sep = "\n", useBytes = FALSE)
# chunk 2
obcpos02 <- POStag(object = pos02)
obcpos02 <- as.vector(unlist(obcpos02))
writeLines(obcpos02, con = here::here("data", "obcpos02.txt"), sep = "\n", useBytes = FALSE)
# chunk 03
obcpos03 <- POStag(object = pos03)
obcpos03 <- as.vector(unlist(obcpos03))
writeLines(obcpos03, con = here::here("data", "obcpos03.txt"), sep = "\n", useBytes = FALSE)
# chunk 04
obcpos04 <- POStag(object = pos04)
obcpos04 <- as.vector(unlist(obcpos04))
writeLines(obcpos04, con = here::here("data", "obcpos04.txt"), sep = "\n", useBytes = FALSE)
# chunk 05
obcpos05 <- POStag(object = pos05)
obcpos05 <- as.vector(unlist(obcpos05))
writeLines(obcpos05, con = here::here("data", "obcpos05.txt"), sep = "\n", useBytes = FALSE)
# chunk 06
obcpos06 <- POStag(object = pos06)
obcpos06 <- as.vector(unlist(obcpos06))
writeLines(obcpos06, con = here::here("data", "obcpos06.txt"), sep = "\n", useBytes = FALSE)
# chunk 07
obcpos07 <- POStag(object = pos07)
obcpos07 <- as.vector(unlist(obcpos07))
writeLines(obcpos07, con = here::here("data", "obcpos07.txt"), sep = "\n", useBytes = FALSE)
# chunk 08
obcpos08 <- POStag(object = pos08)
obcpos08 <- as.vector(unlist(obcpos08))
writeLines(obcpos08, con = here::here("data", "obcpos08.txt"), sep = "\n", useBytes = FALSE)
# chunk 09
obcpos09 <- POStag(object = pos09)
obcpos09 <- as.vector(unlist(obcpos09))
writeLines(obcpos09, con = here::here("data", "obcpos09.txt"), sep = "\n", useBytes = FALSE)
# chunk 10
obcpos10 <- POStag(object = pos10)
obcpos10 <- as.vector(unlist(obcpos10))
writeLines(obcpos10, con = here::here("data", "obcpos10.txt"), sep = "\n", useBytes = FALSE)
# chunk 11
obcpos11 <- POStag(object = pos11)
obcpos11 <- as.vector(unlist(obcpos11))
writeLines(obcpos11, con = here::here("data", "obcpos11.txt"), sep = "\n", useBytes = FALSE)
# chunk 12
obcpos12 <- POStag(object = pos12)
obcpos12 <- as.vector(unlist(obcpos12))
writeLines(obcpos12, con = here::here("data", "obcpos12.txt"), sep = "\n", useBytes = FALSE)
# chunk 13
obcpos13 <- POStag(object = pos13)
obcpos13 <- as.vector(unlist(obcpos13))
writeLines(obcpos13, con = here::here("data", "obcpos13.txt"), sep = "\n", useBytes = FALSE)
```



```{r ampobc_1_07, message=F, warning=F}
# list pos tagged elements
postag.files = c(here::here("data", "obcpos01.txt"), here::here("data", "obcpos02.txt"), 
                 here::here("data", "obcpos03.txt"), here::here("data", "obcpos04.txt"), 
                 here::here("data", "obcpos05.txt"), here::here("data", "obcpos06.txt"),  
                 here::here("data", "obcpos07.txt"), here::here("data", "obcpos08.txt"), 
                 here::here("data", "obcpos09.txt"), here::here("data", "obcpos10.txt"),
                 here::here("data", "obcpos11.txt"), here::here("data", "obcpos12.txt"),
                 here::here("data", "obcpos13.txt"))
# load pos tagged elements
obcpos <- sapply(postag.files, function(x) {
  x <- scan(x, what = "char", sep = "\n", quote = "", quiet = T, skipNul = T)
  x <- stringr::str_squish(x)
  x <- str_replace_all(x, fixed("\n"), " ")
})
# unlist pos tagged elements
obcdf$TextPOS <- as.vector(unlist(obcpos))
```

Save data to disc

```{r ampobc_1_07, message=F, warning=F}
# save data to disc
write.table(obcdf, here::here("data", "obcdf_postagged.txt"), sep = "\t", row.names = F, col.names = T)
obcdf <- read.delim(here::here("data", "obcdf_postagged.txt"), sep = "\t", header = T, skipNul = T)
```

## Concordancing{-}

```{r ampobc_1_08, message=F, warning=F}
# extract number of adjs per line
pstggd <- obcdf$TextPOS
lpstggd <- strsplit(pstggd, " ")
nlpstggd <- as.vector(unlist(sapply(lpstggd, function(x){
  x <- x[grep("[A-Z]{0,1}[a-z]{1,}\\/JJ[A-Z]{0,1}", x)]
  x <- length(x) } )))
rp <- nlpstggd
rp <- ifelse(rp == 0, 1, rp)
# detach dplyr package (clash with plyr)
detach("package:dplyr", unload=TRUE)
# load function for concordancing
source("D:\\R/ConcR_2.3_loadedfiles.R")
# set parameters for concordancing
pattern <- "[A-Z]{0,1}[a-z]{1,}\\/JJ[A-Z]{0,1}"
context <- 50
# extract all adjectives (concordance)
concjjobc <- ConcR(obcdf$TextPOS, pattern, context, all.pre = FALSE)
# repeat rows in data frame as often as there are adjectives in it 
# (if 0 adj, repeat once)
obcadjdf <- obcdf[rep(seq(nrow(obcdf)), rp),]
# combine data sets
obcadjdf <- data.frame(1:nrow(obcadjdf), obcadjdf, concjjobc)
# remove rows without Tokens
obcadjdf <- obcadjdf[is.na(obcadjdf$Token) == F,]
# add clean column names
colnames(obcadjdf)[1] <- "ID"
# clean adjectives
obcadjdf$Adjective <- str_replace_all(obcadjdf$Token, "/.*", "")
# add Variant column
obcadjdf$Variant <- gsub(".* ", "", str_trim(obcadjdf$PreContext, side = "both")) 
# inspect data
head(obcadjdf)
```



```{r ampobc_1_09, message=F, warning=F}
# save raw data to disc
write.table(obcadjdf, here::here("data", "obcadjdf.txt"), sep = "\t", row.names = F, col.names = T)
obcadjdf <- read.delim(here::here("data", "obcadjdf.txt"), sep = "\t", header = T, skipNul = T)
```

## Process sociodemographic data{-}

```{r ampobc_1_10, message=F, warning=F}
# load files
bioobc <- read.delim(bioobcpath, sep = "\t", header = F, skipNul = T)
guideobc <- read.delim(bioguideobc, sep = "\t", header = F, skipNul = T)
linkobc <- read.delim(biolinkobc, sep = "\t", header = F, skipNul = T)
extractsobc <- read.delim(bioextractsobc, sep = "\t", header = F, skipNul = T)
# add column names
bioobc <- bioobc %>%
  dplyr::rename(Id = V1, Ethnicity = V2, Gender = V3, Age = V4, 
                Occupation = V5, Education = V6, L1 = V7) %>%
  mutate(Gender=replace(Gender, Gender=="F", "Woman")) %>%
  mutate(Gender=replace(Gender, Gender=="M", "Man"))
# add column names to guide
linkobc <- linkobc %>%
  dplyr::rename(File = V1, Id = V2, Speaker = V3, Words = V4)
# add column names to extracts
extractsobc <- extractsobc %>%
  dplyr::rename(File = V1, WordsFile = V2, Date = V3, Topic = V4, 
                ExtractMinutes = V5, Minutes = V6)
# join bioobc and linkobc
obcspeakerinformation <- dplyr::left_join(bioobc, linkobc, by = "Id")
obcspeakerinformation <- dplyr::left_join(obcspeakerinformation, extractsobc, by = "File")
# inspect
head(obcspeakerinformation)
```



```{r ampobc_1_11, message=F, warning=F}
# add information about type
obcspeakerinformation <- obcspeakerinformation %>%
  dplyr::mutate(Type = File) %>%
  dplyr::mutate(Type = Type %>%
  str_remove_all("[0-9]{0,}") %>%
  str_replace_all("MSN", "BroadcastNews") %>%
  str_replace_all("MST", "BroadcastMonologue") %>%
  str_replace_all("MSW", "BroadcastWeather") %>%
  str_replace_all("MUC", "SportsCommentary") %>%
  str_replace_all("MUJ", "JudgesSummation") %>%
  str_replace_all("MUL", "Lecture") %>%
  str_replace_all("MUS", "TeacherMonologue") %>%
  str_replace_all("DPC", "Conversation") %>%
  str_replace_all("DPF", "TelephoneConversation") %>%
  str_replace_all("DPH", "OralHistoryInterview") %>%
  str_replace_all("DPP", "SocialDialectInterview") %>%
  str_replace_all("DGB", "RadioTalkback") %>%
  str_replace_all("DGI", "BroadcastInterview") %>%
  str_replace_all("DGU", "ParliamentaryDebate") %>%
  str_replace_all("DGZ", "TransactionsAndMeetings"))
# inspect speaker information
head(obcspeakerinformation); nrow(obcspeakerinformation)
```



```{r ampobc_1_12, message=F, warning=F}
# save raw data to disc
write.table(obcspeakerinformation, here::here("data", "obcspeakerinformation.txt"), sep = "\t", 
            row.names = F, col.names = T)
obcspeakerinformation <- read.delim(here::here("data", "obcspeakerinformation.txt"), sep = "\t", 
                                    header = T, skipNul = T)
obcadjdf <- read.delim(here::here("data", "obcadjdf.txt"), sep = "\t", header = T, skipNul = T)
```

## Combine data with biodata{-}

```{r ampobc_1_13, message=F, warning=F}
# join obcadjdf and obcspeakerinformation
obc <- join(obcadjdf, obcspeakerinformation, by = c("File", "Speaker"))
```

## Frequency{-}

```{r ampobc_1_14, message=F, warning=F}
# code freq of adj type by date category
frqadjtb <- table(obc$Age, obc$Adjective)
relfreqadjtb <- round(prop.table(frqadjtb, margin = 1)*100, 5)
relfreqadjdf <- as.data.frame(relfreqadjtb)
colnames(relfreqadjdf)[1:2] <- c("Age", "Adjective")
# add freq by date to data
obcadjdf <- join(obc, relfreqadjdf, by=c("Age", "Adjective"))
# relabel Freq
colnames(obcadjdf)[which(colnames(obcadjdf) == "Freq")] <- "Frequency"
# inspect data
head(obcadjdf)
```

Save data to disc

```{r ampobc_1_15, message=F, warning=F}
# save raw data to disc
write.table(obcadjdf, here::here("data", "obcadjdf_spk.txt"), sep = "\t", row.names = F, col.names = T)
obcadjdf <- read.delim(here::here("data", "obcadjdf_spk.txt"), sep = "\t", header = T, skipNul = T)
```

## Check Amplifiers{-}

not included: *quite*, *too*, and *well*.

```{r ampobc_1_16, message=F, warning=F}
# define amplifiers 
amplifiers <- c("absolutely", "actually", "aggressively", "amazingly", "appallingly", "awful", 
                "awfully", "badly", "bloody", "certainly", "clearly", "complete", "dead", "completely", 
                "considerably", "crazy", "decidedly", "definitely", "distinctly", "dreadfully", 
                "enormously", "entirely", "especially", "exactly", "exceedingly", "exceptionally", 
                "excruciatingly", "extraordinarily", "extremely", "fiercely", "firmly", "frightfully", 
                "fucking", "fully", "genuinely", "greatly", "grossly", "heavily", "highly", "hopelessly",
                "horrendously", "hugely", "immediately", "immensely", "incredibly", "infinitely", 
                "intensely", "irrevocably", "mad", "mega", "mighty", "most", "much", "obviously", 
                "openly", "overwhelmingly", "particularly", "perfectly", "plenty", "positively", 
                "precisely", "pretty", "profoundly", "purely", "real", "really", "remarkably", "seriously",
                "shocking", "significant", "significantly", "so", "specially", "specifically", "strikingly", 
                "strongly", "substantially", "super", "surely", "terribly", "terrifically", "total", 
                "totally", "traditionally", "true", "truly", "ultra", "utterly", "very", "viciously", 
                "wholly", "wicked", "wildly")
```

## Syntactic Function{-}

```{r ampobc_1_17, message=F, warning=F}
# determine function
Function1 <- obcadjdf$PostContext %>%
  stringr::str_trim(side = "both") %>%
  tolower() %>%
  stringr::str_replace_all(" {2,}", " ") %>%
  stringr::str_replace_all(" .*", "") %>%
  stringr::str_replace_all(" .*", "") %>%
  stringr::str_replace_all(".*/n.*", "Attributive") %>%
  stringr::str_replace_all(fixed("/."), "PUNCT") %>%
  stringr::str_replace_all(".*PUNCT.*", "Attributive")
Function2 <-  obcadjdf$PostContext %>%
  stringr::str_trim(side = "both") %>%
  tolower() %>%
  stringr::str_replace(" ", "") %>%
  stringr::str_replace_all(" .*", "") %>%
  stringr::str_replace_all(".*/n.*", "Attributive") %>%
  stringr::str_replace_all(fixed("/."), "PUNCT") %>%
  stringr::str_replace_all(".*PUNCT.*", "Attributive")
functiontb <- cbind(Function1, Function2)
obcadjdf$Function <-  as.vector(unlist(apply(functiontb, 1, function(x){
  x <-   ifelse(x[1] == "" | x[1] == "Attributive" | x[2] == "Attributive", "Attributive", "Predicative")
})))
# shorten post Context
obcadjdf$PostContext <- substr(obcadjdf$PostContext, 1, ifelse((nchar(obcadjdf$PostContext)+25) <25, max(nchar(obcadjdf$PostContext)), 25))
# pre Context
obcadjdf$PreContext <- str_trim(obcadjdf$PreContext, side = "both")
obcadjdf$PreContextLong <- obcadjdf$PreContext
obcadjdf$PreContextLong <- substr(obcadjdf$PreContextLong, ifelse(nchar(obcadjdf$PreContextLong)-25 <=0, 1, 
                                                              nchar(obcadjdf$PreContextLong)-25), nchar(obcadjdf$PreContextLong))
obcadjdf$PreContext <- gsub(".* ", "", obcadjdf$PreContext)
# amplifier variant
obcadjdf$PreContext <- gsub("\\/.*", "", obcadjdf$PreContext)
obcadjdf$Variant <- ifelse(obcadjdf$PreContext %in% amplifiers, obcadjdf$PreContext, "0")
# amplified y/n
obcadjdf$Amplified <- ifelse(obcadjdf$Variant == "0", 0, 1) 
# adjective
obcadjdf$Adjective <- tolower(obcadjdf$Adjective)
# inspect data
nrow(obcadjdf); head(obcadjdf); table(obcadjdf$Variant)
```

## Code Priming{-}

```{r ampobc_1_18, message=F, warning=F}
# code priming
prim1 <- c(rep(0, 1), obcadjdf$Variant[1:length(obcadjdf$Variant)-1])
prim2 <- c(rep(0, 2), obcadjdf$Variant[1:(length(obcadjdf$Variant)-2)])
prim3 <- c(rep(0, 3), obcadjdf$Variant[1:(length(obcadjdf$Variant)-3)])
primtb <- cbind(obcadjdf$Variant, prim1, prim2, prim3)

obcadjdf$Priming <- as.vector(unlist(apply(primtb, 1, function(x){
  x <- ifelse(x[1]== "0" , "NoPrime",
              ifelse(x[1] == x[2] | x[1] == x[3] | x[1] == x[4], "Prime", "NoPrime"))
})))
# define forms that require removal
sups <- c(".*most.*", ".*more.*") 
negs <- c(".*not.*", ".*never.*", ".*n't.*")
downtoners <- c(".*sort/.*", ".*kind/.*", ".* bit/.*", ".*somewhat.*", ".*fairly.*", 
                ".*rather.*", ".*reasonably.*", ".*slightly.*", ".*comparatively.*", ".*semi.*", 
                ".*relatively.*", ".*little.*", ".*somehow.*", ".*almost.*", ".*partly.*", 
                ".*hardly.*", ".* less.*", ".*barely.*", ".* just/.*")
specialforms <- c(".* too.*", ".*quite.*")
PostContextdowntoners <- c(".*enough.*")
nonpropadj <- c("only", "much", "many", "cheaper", "cheaperr", "bests", "larger",
                "bst", "better", "bigger")
# check length of dataset
str(obcadjdf); head(obcadjdf); nrow(obcadjdf)
```

## Remove negated forms et al.{-}

```{r ampobc_1_19, message=F, warning=F}
# find items to be removed
supsidx <- unique(grep(paste(sups,collapse="|"), obcadjdf$PreContextLong, value=F))
negsidx <- unique(grep(paste(negs,collapse="|"), obcadjdf$PreContextLong, value=F))
downtonersidx <- unique(grep(paste(downtoners,collapse="|"), obcadjdf$PreContextLong, value=F))
specialformsidx <- unique(grep(paste(specialforms,collapse="|"), obcadjdf$PreContextLong, value=F))
PostContextdowntonersidx <- unique(grep(paste(PostContextdowntoners,collapse="|"), obcadjdf$PostContext, value=F))
nonpropadjidx <- unique(grep(paste(nonpropadj,collapse="|"), obcadjdf$Adjective, value=F))
# combine indices
idxs <- unique(c(supsidx, negsidx, downtonersidx, specialformsidx, PostContextdowntonersidx, nonpropadjidx))
# remove forms that require removal
obcadjdf <- obcadjdf[-idxs,]
# remove empty values
obcadjdf <- obcadjdf[!obcadjdf$Variant == "", ]
```



```{r ampobc_1_20, message=F, warning=F}
# save raw data to disc
write.table(obcadjdf, here::here("data", "obcadjdf_wo_neg.txt"), sep = "\t", row.names = F)
obcadjdf <- read.delim(here::here("data", "obcadjdf_wo_neg.txt"), sep = "\t", header = T, skipNul = T)
```

## Remove lexicalizations 

Remove variants that were not intensified by at least 2 different amplifier variants

> NOT EXECTUTED!

```{r ampobc_1_21, message=F, warning=F, eval = F}
nrow(obcadjdf)
# remove items that were not intensified by a minimum of 2 intensifier variants
pintadjtb <- table(obcadjdf$Adjective, obcadjdf$Variant)
#pintadjtb <- pintadjtb[2:nrow(pintadjtb),]
pintadjtb <- pintadjtb[,2:ncol(pintadjtb)]
pintadjtb2 <- apply(pintadjtb, 1, function(x){
  x <- ifelse(x > 1, 1, x)})
pintadjtb3 <- colSums(pintadjtb2)
pintadjschildes <- names(pintadjtb3)[which(pintadjtb3 >= 2)]
obcadjdf <- obcadjdf[obcadjdf$Adjective %in% pintadjschildes, ]
nrow(obcadjdf)
```

## Remove misspelled forms {-}

```{r ampobc_1_22, message=F, warning=F, eval = F}
# inspect adjectives
names(table(obcadjdf$Adjective))
```

Remove words not in dictionary or words that consist only of 1 or 2 characters

```{r ampobc_1_23, message=F, warning=F}
# correct spelling
library(hunspell)
obcadjdf <- obcadjdf %>%
  dplyr::filter(hunspell_check(Adjective)) %>%
  dplyr::filter(nchar(Adjective) > 2)
# inspect
head(obcadjdf)
```



## Remove strange forms{-}

```{r ampobc_1_24, message=F, warning=F}
# inspect adjectives
names(table(obcadjdf$Adjective))
```

Remove mistagged items

```{r ampobc_1_25, message=F, warning=F}
library(udpipe)
#model <- udpipe_download_model(language = "english")
udmodel_english <- udpipe_load_model(file = here::here("english-ewt-ud-2.5-191206.udpipe"))
udpipe_adjectives <- udpipe_annotate(udmodel_english, obcadjdf$Adjective) %>%
  data.frame() %>%
  dplyr::filter(xpos == "JJ") %>%
  dplyr::pull(sentence)
# inspect
head(udpipe_adjectives)
```

```{r ampobc_1_26, message=F, warning=F}
nrow(obcadjdf)
obcadjdf <-obcadjdf %>%
  dplyr::filter(Adjective %in% udpipe_adjectives)
# inspect
nrow(obcadjdf)
```

Remove adjectives that were never amplified


```{r ampobc_1_27, message=F, warning=F}
amp_adjs <- obcadjdf %>%
  dplyr::filter(Amplified == 1) %>%
  dplyr::pull(Adjective) %>%
  table() %>%
  names()
# inspect
head(amp_adjs)
```
```{r ampobc_1_28, message=F, warning=F}
nrow(obcadjdf)
obcadjdf <- obcadjdf %>%
  dplyr::filter(Adjective %in% amp_adjs)
nrow(obcadjdf)
```


Save data to disc

```{r ampobc_1_29, message=F, warning=F}
# save raw data to disc
write.table(obcadjdf, here::here("data", "obcadjdf_clean.txt"), sep = "\t", row.names = F)
obcadjdf <- read.delim(here::here("data", "obcadjdf_clean.txt"), sep = "\t", header = T, skipNul = T)
```

## Gradability{-}

```{r ampobc_1_30, message=F, warning=F}
# code gradability
# load Gradability data (derived from COCA)
gradability <- read.delim(here::here("data", "Gradability.txt"), sep = "\t", header = T, quote = "", skipNul = T)
obcadjdf$Gradability <- ifelse(obcadjdf$Adjective %in% gradability$Adjective, gradability$Beta, 1)
# inspect data
nrow(obcadjdf); head(obcadjdf)
```

## Semantic Classification{-}

Add semantic types [@tagliamonte2008intensifiers, @dixon1977adjectives]

* dimension = semdim (e.g. big, large, little, small, long, short, wide, narrow, thick)

* difficulty = semdif (e.g. difficult, simple)

* physical property = (e.g. hard, soft, heavy, light, rough, smooth, hot, sweet)

* color = semcol (e.g. black, white, red)

* human propensity: semhup (e.g. jealous, happy, kind, clever, generous, gay, rude)

* age = semage (e.g. new, young, old) 

* value (e.g. good, bad, proper, perfect, excellent, delicious, poor), 

* speed Speed (fast, quick, slow)

* position (e.g. right, left, near, far)

* other

```{r ampobc_1_31, message=F, warning=F}
# load data
code1 <- read.delim(here::here("data", "semcodecg1.txt"), sep = "\t", header = T, skipNul = T)
code2 <- read.delim(here::here("data", "semcodedjm1.txt"), sep = "\t", header = T, skipNul = T)
code3 <- read.delim(here::here("data", "semcodedl1.txt"), sep = "\t", header = T, skipNul = T)
code4 <- read.delim(here::here("data", "semcodedm1.txt"), sep = "\t", header = T, skipNul = T)
code5 <- read.delim(here::here("data", "semcodedma1.txt"), sep = "\t", header = T, skipNul = T)
code6 <- read.delim(here::here("data", "semcodedv1.txt"), sep = "\t", header = T, skipNul = T)
code7 <- read.delim(here::here("data", "semcodedjw1.txt"), sep = "\t", header = T, skipNul = T)
# order data sets
code1 <- code1[order(code1$Id),]
code2 <- code2[order(code2$Id),]
code3 <- code3[order(code3$Id),]
code4 <- code4[order(code4$Id),]
code5 <- code5[order(code5$Id),]
code6 <- code6[order(code6$Id),]
code7 <- code6[order(code7$Id),]
# repair adjectives in code1
code3$Adjective <- code1$Adjective
# combine tables
semcode <- rbind(code1, code2, code3, code4, code5, code6, code7)
# convert coding into numeric values
semcode[,3:12] <- t(apply(semcode[,3:12], 1, function(x) {
  x <- ifelse(x == "" | x == "?"| is.na(x) == T, 0, 1)}))
# convert into data frame
semcode <- as.data.frame(semcode)
# add column names
colnames(semcode)[3:12] <- c("Dimension", "Difficulty", "PhysicalProperty", "Color",
                             "HumanPropensity", "Age", "Value", "Speed", "Position", 
                             "Other")
```


```{r ampobc_1_32, message=F, warning=F}
# load package
library(dplyr)
AdjectiveSemantics <- semcode %>%
  dplyr::group_by(Adjective) %>%
  na.omit() %>%
  dplyr::summarize(Dimension = sum(Dimension), 
                   Difficulty = sum(Difficulty),
                   PhysicalProperty = sum(PhysicalProperty), 
                   Color = sum(Color),
                   HumanPropensity = sum(HumanPropensity), 
                   Age = sum(Age),
                   Value = sum(Value), 
                   Speed = sum(Speed),
                   Position = sum(Position), 
                   Other = sum(Other)) %>%
  dplyr::mutate(OverallScore = rowSums(.[,2:11])) %>%
  dplyr::mutate(Maximum = do.call(pmax, (.[,2:11]))) %>%
  dplyr::mutate(Certainty = Maximum/OverallScore*100)
AdjectiveSemantics <- AdjectiveSemantics %>%
  dplyr::mutate(Category = colnames(AdjectiveSemantics[2:11])[apply(AdjectiveSemantics[2:11],1,which.max)])
# inspect interrater reliability
summary(AdjectiveSemantics$Certainty)
```

```{r ampobc_1_33, message=F, warning=F}
Age <- AdjectiveSemantics %>%
  dplyr::filter(Age != 0) %>%
  dplyr::pull(Adjective)
Color <- AdjectiveSemantics %>%
  dplyr::filter(Color != 0) %>%
  dplyr::pull(Adjective)
Difficulty <- AdjectiveSemantics %>%
  dplyr::filter(Difficulty != 0) %>%
  dplyr::pull(Adjective)
Dimension <- AdjectiveSemantics %>%
  dplyr::filter(Dimension != 0) %>%
  dplyr::pull(Adjective)
HumanPropensity <- AdjectiveSemantics %>%
  dplyr::filter(HumanPropensity != 0) %>%
  dplyr::pull(Adjective)
PhysicalProperty <- AdjectiveSemantics %>%
  dplyr::filter(PhysicalProperty != 0) %>%
  dplyr::pull(Adjective)
Position <- AdjectiveSemantics %>%
  dplyr::filter(Position != 0) %>%
  dplyr::pull(Adjective)
Speed <- AdjectiveSemantics %>%
  dplyr::filter(Speed != 0) %>%
  dplyr::pull(Adjective)
Value <- AdjectiveSemantics %>%
  dplyr::filter(Value != 0) %>%
  dplyr::pull(Adjective)
# add semantic category to data
obcadjdf <- obcadjdf %>%
  dplyr::mutate(SemanticCategory = dplyr::case_when(Adjective %in% Age ~ "Age",
                                                    Adjective %in% Color ~ "Color",
                                                    Adjective %in% Difficulty ~ "Difficulty",
                                                    Adjective %in% Dimension ~ "Dimension",
                                                    Adjective %in% HumanPropensity ~ "HumanPropensity",
                                                    Adjective %in% PhysicalProperty ~ "PhysicalProperty",
                                                    Adjective %in% Position ~ "Position",
                                                    Adjective %in% Speed ~ "Speed",
                                                    Adjective %in% Value ~ "Value", 
                                                    TRUE ~ "Other"))
# table sem class of tokens
table(obcadjdf$SemanticCategory)
```


## Sentiment Analysis{-}

```{r ampobc_1_34, message=F, warning=F}
# positive adjectives
pos_adj <- tidytext::get_sentiments("nrc") %>%
  dplyr::filter(sentiment == "positive") %>%
  dplyr::pull(word)
# negative adjectives
neg_adj <- tidytext::get_sentiments("nrc") %>%
  dplyr::filter(sentiment == "negative") %>%
  dplyr::pull(word)
# annotate emotionality
obcadjdf <- obcadjdf %>%
  dplyr::mutate(Emotionality = dplyr::case_when(Adjective %in% pos_adj ~ "positive",
                                                Adjective %in% neg_adj ~ "negative",
                                                TRUE ~ "neutral")) %>%
  dplyr::mutate(Emotionality = factor(Emotionality, levels = c("neutral", "negative", "positive")))
# inspect
head(obcadjdf)
```

## Correct errors{-}

```{r ampobc_1_35, message=F, warning=F}
obcadjdf %>%
  dplyr::filter(Variant == "so") %>%
  dplyr::group_by(Variant, Function) %>%
  dplyr::summarize(Frequency = n())
```


```{r ampobc_1_36, message=F, warning=F}
obcadjdf <- obcadjdf %>%
  dplyr::mutate(Function = ifelse(Variant == "so" & Function == "Attributive", "Predicative", Function))
```

```{r ampobc_1_37, message=F, warning=F}
obcadjdf %>%
  dplyr::filter(Variant == "so") %>%
  dplyr::group_by(Variant, Function) %>%
  dplyr::summarize(Frequency = n())
```

```{r ampobc_1_38, message=F, warning=F}
# inspect data
obcadjdf %>%
  head(10) %>%
  kable(caption = "First 10 rows of obcadjdf.") %>%
  kableExtra::kable_styling(bootstrap_options = c("striped", "hover", "condensed", "responsive"), 
                            full_width = F)
```


```{r ampobc_1_39, message=F, warning=F}
# save raw data to disc
write.table(obcadjdf, here::here("data", "obc_fullclean.txt"), sep = "\t", row.names = F, col.names = T)
```


# Citation & Session Info {-}

Schweinberger, Martin. `r format(Sys.time(), '%Y')`. *On the waning of forms - a corpus-based analysis of losers in language change (Part 1)*. Brisbane: The University of Queensland. url: https://slcladal.github.io/isle6verynze1.html (Version `r format(Sys.time(), '%Y.%m.%d')`).

```
@manual{schweinberger`r format(Sys.time(), '%Y')`isle6verynze1,
  author = {Schweinberger, Martin},
  title = {On the waning of forms - a corpus-based analysis of losers in language change (Part 1)},
  note = {https://slcladal.github.io/isle6verynze1.html},
  year = {`r format(Sys.time(), '%Y')`},
  organization = {The University of Queensland, School of Languages and Cultures},
  address = {Brisbane},
  edition = {`r format(Sys.time(), '%Y.%m.%d')`}
}
```

```{r ampobc_1_40, message=F, warning=F}
sessionInfo()
```


***

[Back to top](#introduction)


***

# References {-}

